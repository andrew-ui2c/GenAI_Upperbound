{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6e63b91-3ac2-43e4-821e-1f3b3c9ccc7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# AI-Powered SQL Functions – Databricks Notebook Exercises\n",
    "\n",
    "Workshop Notebook: AI SQL Functions – Building AI into Your SQL \n",
    "Workflows \n",
    "\n",
    "In this notebook, you will explore Databricks AI Functions – a set of built-in SQL functions that integrate Large Language Models (LLMs) directly into SQL queries. This enables powerful text analysis and generation capabilities within your database (no separate API calls or pipelines required). You’ll practice using these functions on a practical scenario: analyzing customer reviews and automating responses, all in SQL. The notebook is structured with explanatory markdown cells and interactive exercises. Work through each section, running the SQL commands and observing the results. Feel free to experiment with your own inputs as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0d11557-cdae-471b-bef0-e8c4e8fd64f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Built-In AI Functions Quick Tour\n",
    "\n",
    "First, familiarize yourself with a few key AI functions provided by Databricks SQL. These functions are pre-trained on language tasks and run on hosted models:\n",
    "- AI_ANALYZE_SENTIMENT(text) – Analyzes the sentiment of the input text, returning a label such as “POSITIVE”, “NEGATIVE”, or “NEUTRAL”.\n",
    "- AI_CLASSIFY(text, labels_array) – Classifies the input text into one of the given categories. You provide an array of label strings (e.g., array('Spam','Not Spam')), and it returns the label that best fits the text.\n",
    "- AI_SUMMARIZE(text) – Produces a concise summary of the input text.\n",
    "- AI_EXTRACT(text, labels…) – Extracts specified information from the text (returns a JSON with those fields).\n",
    "- (There are additional functions, but we’ll focus on these core ones for now.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51d76470-875c-42f9-a223-6ff209e71538",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 1: Sentiment Analysis of a Single Phrase\n",
    "Let’s start with a simple sentiment analysis. Run the query below to test AI_ANALYZE_SENTIMENT on a sample sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63ef22dc-b295-43b6-b787-4230c087974e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- What sentiment do we get for a clearly positive statement?\n",
    "SELECT ai_analyze_sentiment('I absolutely love the new coffee blend! It tastes wonderful.') AS sentiment;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5171fc0-2cf0-43be-92bd-f10a8c71c81d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After running, observe the output. The result should be a single value in the “sentiment” column, likely POSITIVE (since the sentence is clearly positive).\n",
    "\n",
    "Question: If you change the input text to something negative (for example, “I am really unhappy with the service.”), what sentiment is returned? Try it out by editing the string below. This illustrates how the function picks up on positive vs. negative tone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d043b1d3-6242-4677-8519-71ddff00d434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- What sentiment do we get for a clearly positive statement?\n",
    "SELECT ai_analyze_sentiment('YOUR INPUT HERE') AS sentiment;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f014570-a594-4e2e-80d3-d8d7c8998c72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 2: Classifying Text into Categories\n",
    "Now, test the AI_CLASSIFY function. We will classify a piece of feedback as either a Complaint or a Praise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "420d0f1d-6e27-4b14-a2ce-da9d5271b9e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Classify the tone of the feedback as 'Complaint' or 'Praise'\n",
    "SELECT ai_classify(\n",
    "    'The product broke after one use and I am very disappointed.',\n",
    "    array('Complaint', 'Praise')\n",
    "  ) AS feedback_type;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7f06b46-c05d-4e91-93c5-ebc503569180",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Your task: Run the above query and check the output. It should return either \"Complaint\" or \"Praise\". Given the input text (which is clearly negative), we expect \"Complaint\".\n",
    "\n",
    "To confirm, you can also try a positive feedback example. For instance, change the text to “This cake was absolutely delicious and the service was excellent” and run again – you should see \"Praise\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d61f7852-a21a-4da9-bfc9-0bc447aa587f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Classify the tone of the feedback as 'Complaint' or 'Praise'\n",
    "SELECT ai_classify(\n",
    "    'YOUR TEXT HERE',\n",
    "    array('Complaint', 'Praise')\n",
    "  ) AS feedback_type;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3628473a-01a0-4274-b650-28231216e5cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise 3: Summarizing a Customer Review\n",
    "Next, let’s try out AI_SUMMARIZE. Imagine we have a long customer review, and we want a brief summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b33fe14-f783-43d7-ba0f-2c1aa3e7ee6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Summarize a verbose customer review\n",
    "SELECT ai_summarize(\n",
    "    'I visited the bakery yesterday. The ambiance was nice, and the staff was friendly. \n",
    "     I tried the new croissant and it was flaky and buttery, absolutely perfect! \n",
    "     I will be coming back for more, even though the coffee was a bit too strong for my taste.'\n",
    "  ) AS review_summary;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7730ce39-ef26-412f-a296-3a5828f5b73a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the query and examine the review_summary output. It should condense the multi-sentence review into a shorter sentence or two, capturing the key points (e.g., positive about ambiance and croissant, minor issue with coffee strength).\n",
    "\n",
    "Feel free to modify the input text or write your own long-ish review to see how well the summarization captures the essence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61dfe242-a702-4239-a3c4-3506a11bfb98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Summarize a verbose customer review\n",
    "SELECT ai_summarize(\n",
    "    'YOUR LONG-ISH REVIEW HERE'\n",
    "  ) AS review_summary;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61d0e744-8d2b-4929-a5f2-b8718ca762ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Scenario: Automating Customer Review Analysis with AI\n",
    "\n",
    "Now that you’ve gotten a feel for individual AI functions, let’s apply them in a realistic scenario. UpperBound Bakehouse receives customer reviews (from surveys or online forms), and we want to automate the analysis of these reviews to improve customer satisfaction. Specifically, for each review we aim to:\n",
    "1. Determine the sentiment (positive/negative).\n",
    "2. Decide if the review indicates the customer requires a follow-up from support.\n",
    "3. If a follow-up is needed (for negative experiences), generate a suggested response message addressing the feedback.\n",
    "\n",
    "To make this hands-on, we’ll generate some sample customer reviews using an LLM, then use AI Functions to analyze them. Finally, we’ll produce an automated response for the negative ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25c54f4e-d050-4325-bbf5-b032b8318750",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.1 Generating Sample Reviews with AI_QUERY\n",
    "First, we need some example reviews to work with. We’ll use the AI_QUERY function to generate synthetic review text. AI_QUERY allows us to call a specific model endpoint and get its response directly in SQL. We will use one of Databricks’ foundation models as our LLM.\n",
    "\n",
    "### Exercise 4: Create a Table of Fake Reviews\n",
    "Run the following to generate, say, 5 sample reviews. We use a range(5) trick to produce 5 rows, each prompting the model for a new review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9f58e81-15fc-4246-ac05-8714da16d199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Generate 5 unique synthetic customer reviews using an LLM via AI_QUERY\n",
    "CREATE OR REPLACE TEMPORARY VIEW sample_reviews AS\n",
    "SELECT explode(split(ai_query(\n",
    "         \"databricks-meta-llama-3-3-70b-instruct\", \n",
    "         'Generate 5 unique short customer reviews for bakery products. \n",
    "          Include specific details; if negative, mention the issue. \n",
    "          Vary the tone across the reviews (some positive, some negative).\n",
    "          Only include the text of the review. No need for the Here are 5 unique short customer reviews for bakery products: at the beginning\n",
    "          NO need for the rating or name. I just want the texts of the reviews'\n",
    "       ), '\\n\\n')) AS review_text;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0055edd4-13df-4aee-bcdc-4ce51f31e5a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This uses the databricks-meta-llama-3-3-70b-instruct model endpoint to generate five different review texts. We store them in a temporary view sample_reviews.\n",
    "\n",
    "After creation, quickly preview the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "404a6916-ac1c-4f4b-b571-3f3d0051730c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM sample_reviews;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39ad7a54-c000-4022-af03-4b73c98a123a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You should see five rows of fairly realistic-sounding review text (e.g., “I tried the chocolate muffin and it was stale...”, “The new seasonal pie was fantastic...”, etc.). Each row is just a single column review_text. Keep these reviews in mind as we proceed. \n",
    "\n",
    "**Question:** Do the generated reviews include a mix of sentiments? Ideally, yes – some should be positive and some negative or neutral. If they all look similar or too positive, consider regenerating with a slightly tweaked prompt emphasizing varied sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "893ac569-b4d1-4909-bf50-5a8365169ed1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.2 Analyzing Reviews: Sentiment and Follow-Up Requirement\n",
    "With our sample reviews in hand, let’s apply AI functions to extract useful information.\n",
    "\n",
    "### Exercise 5: Analyze Sentiment and Flag for Follow-Up\n",
    "We’ll create another view that augments each review with two new pieces of information: the sentiment, and a follow-up flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe64ed80-6a75-4772-a0f7-251a2044b5f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Analyze each review: get sentiment and determine if follow-up is needed\n",
    "CREATE OR REPLACE TEMPORARY VIEW reviewed_analysis AS\n",
    "SELECT \n",
    "  review_text,\n",
    "  ai_analyze_sentiment(review_text) AS sentiment,\n",
    "  ai_classify(\n",
    "      review_text, \n",
    "      array('Follow-up Needed', 'No Follow-up Needed')\n",
    "    ) AS followup_status\n",
    "FROM sample_reviews;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67c07022-cc48-4a95-abe7-8daa38c81296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the above. This query goes through each review_text and:\n",
    "- assigns a sentiment (Positive/Negative/Neutral),\n",
    "- classifies the review into either \"Follow-up Needed\" or \"No Follow-up Needed\". We chose these two labels to indicate whether the feedback is bad enough that customer support should reach out.\n",
    "\n",
    "Now, inspect the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49beb4d5-7709-4b68-b8e1-edd969112cc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM reviewed_analysis;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e22f5e3-3426-4a5e-9f38-52aeb9d03aed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You’ll see each review with a sentiment and a follow-up status. For example, a very negative review might show sentiment = NEGATIVE and followup_status = Follow-up Needed, whereas a glowing review would be POSITIVE and No Follow-up Needed.\n",
    "\n",
    "**Questions:**\n",
    "- Do the sentiment labels correctly reflect the tone of the reviews you read earlier? (A quick manual check helps – e.g., a review praising a product should be POSITIVE.)\n",
    "- Which reviews were flagged as needing follow-up? Why do you think the model marked them? Typically, reviews that mention serious issues, dissatisfaction, or defects should be tagged \"Follow-up Needed\". Verify if that matches your expectation for each review.\n",
    "\n",
    "Behind the scenes: AI_CLASSIFY with those two labels is essentially doing a sentiment-based classification, but with a specific business spin: it’s looking for cues of dissatisfaction. The model likely correlates negative sentiment or complaint keywords with \"Follow-up Needed\". This is an example of how you can customize AI functions for business rules by simply changing the labels or phrasing.\n",
    "\n",
    "## 2.3 Generating Suggested Responses for Negative Reviews\n",
    "Finally, for any reviews that require follow-up, we’ll use the LLM to generate a brief response message to the customer. This could save our customer support team time by providing a draft email or reply.\n",
    "\n",
    "### Exercise 6: Generate Follow-up Messages\n",
    "Let’s query only the reviews that were flagged as needing follow-up, and for each, ask the LLM to produce a response. We will use AI_QUERY again on the same model, this time feeding it a prompt that includes the review text and asks for a helpful reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "372cc76c-0e07-4ade-9924-665199119600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Generate a suggested response for each review that needs follow-up\n",
    "SELECT \n",
    "  review_text,\n",
    "  sentiment,\n",
    "  ai_query(\n",
    "    \"databricks-meta-llama-3-3-70b-instruct\",\n",
    "    CONCAT(\n",
    "      \"Compose a brief, polite response to the following customer comment, acknowledging their issue and offering to help: ```\", \n",
    "      review_text, \n",
    "      \"```\"\n",
    "    )\n",
    "  ) AS suggested_response\n",
    "FROM reviewed_analysis\n",
    "WHERE followup_status = 'Follow-up Needed';\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12507dd0-9a8e-4caa-854e-0fb0ec9873ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Task:** Execute the above query. It will take each review marked \"Follow-up Needed\", and send a prompt to the LLM like: \"Compose a brief, polite response to the following customer comment... [customer review].\" We wrap the review text in backticks or quotes to clearly delineate it in the prompt. \n",
    "\n",
    "Results: You should get a table of (review_text, sentiment, suggested_response). The suggested_response will be a few sentences addressing the customer. For example, if the review said a product was stale or service was poor, the response might apologize and offer a replacement or discount: “We’re very sorry to hear about your experience with the muffin. We strive for freshness... Please contact us and we’ll make it right...”. \n",
    "\n",
    "Take a moment to read the suggested responses. They should be polite, empathetic, and address the problem.\n",
    "\n",
    "**QUESTION:** \n",
    "Do these AI-generated messages sound appropriate and helpful? Would you edit them before sending to a real customer? It’s important to critically evaluate AI outputs – in many cases they’re a great starting draft, but a human may need to review for tone or specifics. In our case, they should be fairly on-point, since the prompt explicitly asked for a polite, helpful reply.\n",
    "\n",
    "For completeness, you might also want to see the responses for positive reviews (though we typically wouldn’t follow up on those). You can adjust the WHERE clause to include all reviews (or specifically \"No Follow-up Needed\") just to see what the model would say. It might produce a generic thank-you note for positive comments if asked.\n",
    "\n",
    "## 3. (Optional) Using AI_EXTRACT for Structured Extraction\n",
    "\n",
    "This section is optional and for exploration. Instead of using separate steps for sentiment and classification, Databricks offers an AI_EXTRACT function that can pull out multiple fields from text in one go. If you wanted, you could extract, say, “sentiment” and “issue_type” from a review with one function call.\n",
    "\n",
    "For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48b34d49-4b7e-411b-868b-510853c2c5ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT ai_extract(\n",
    "  review_text, \n",
    "  array('sentiment','issue_type')\n",
    ") AS extract_json\n",
    "FROM sample_reviews;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97b8a30e-0b70-48a4-8b27-ce18981bfe1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This would attempt to return a JSON string like: {\"sentiment\": \"NEGATIVE\", \"followup_reason\": \"defect in product\"} for a negative review complaining about a defect. You would then parse the JSON using JSON functions in SQL. Under the hood, AI_EXTRACT is using the model to fill in the requested labels in a structured format.\n",
    "\n",
    "We won’t rely on this in our main flow above (to keep things simple and separated), but it’s good to know such capabilities exist for more advanced patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da22a258-e2fa-4ae0-aa13-7b72b3654270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Wrap-Up and Next Steps\n",
    "\n",
    "In this notebook, we:\n",
    "- Used built-in AI Functions (ai_analyze_sentiment, ai_classify, ai_summarize) to directly analyze text in SQL, leveraging Databricks-hosted LLMs (no external infrastructure needed).\n",
    "- Employed ai_query to call a specific large language model for custom tasks: generating synthetic data and creating tailored responses. This showcases the flexibility to plug in different models (Databricks Foundation Models, open-source LLMs, or external APIs) via the same SQL interface.\n",
    "- Built a mini pipeline that turns raw unstructured customer reviews into actionable insights (sentiment, follow-up flag) and even auto-generated response drafts – all within a SQL workflow. This is a powerful example of an AI system in the database, useful for customer experience management, support ticket triage, and more.\n",
    "\n",
    "**Discussion:** Think about how you could incorporate these techniques into your own projects. \n",
    "\n",
    "Finally, remember that with great power comes great responsibility:\n",
    "- Always review AI outputs, especially if they will be customer-facing. Quality can vary, and models might occasionally produce irrelevant or incorrect content.\n",
    "- Be mindful of cost and performance – LLM calls are not free or instantaneous. We used small numbers of examples here; in production you’d use batching.\n",
    "- Ensure compliance with data privacy and model usage policies (e.g., don’t send sensitive data to external models without proper handling).\n",
    "\n",
    "You have now completed the AI SQL Functions exercises! Feel free to experiment further in this notebook – perhaps try summarizing all reviews in one query, or use AI_CLASSIFY with more categories (like “Positive”, “Negative”, “Neutral” on the review text to compare with AI_ANALYZE_SENTIMENT). The more you play with these functions, the more ideas you’ll get for applying them in your AI-powered data pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "478f5d5e-b9f7-458b-bc87-9e45db062a56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5605242329892280,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_ai_functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
